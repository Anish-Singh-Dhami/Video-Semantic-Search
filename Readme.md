# ğŸ¥ Intent-Based Video Navigator (Semantic Search for Video)

> A backend-first system that lets users *query inside videos* using natural language â€” like  
> â€œShow me where the speaker explains gradient descent.â€  
> or â€œJump to the part where the demo starts.â€

This project aims to make long-form videos instantly searchable and interactive using **semantic understanding**, **vector search**, and **AI embeddings** â€” all built using **free cloud-first tools** (no credit card required).



## ğŸš€ Current Goal (Milestone 1)

Weâ€™re building the backend that supports **semantic search inside videos**:
- Upload a video or provide a URL.
- Extract and segment its transcript.
- Generate embeddings via **Hugging Face Inference API**.
- Store vectors + metadata in a **cloud vector database** (like Qdrant Cloud).
- Query text like _â€œwhere does the speaker talk about backpropagation?â€_ to get relevant video timestamps.

## ğŸ§  System Overview

**User Flow:**
1. User uploads or links a video.  
2. Server extracts audio and sends it to Whisper for transcription.  
3. Transcript text is chunked into smaller segments.  
4. Each segment is embedded using Hugging Face Inference API.  
5. Embeddings and metadata (timestamps, videoId) are stored in Qdrant.  
6. User sends a query like *"where is gradient descent explained?"*.  
7. Query is embedded and matched against stored segments.  
8. Server returns timestamps and snippet text to frontend.

## ğŸ“ Folder Structure

```
backend/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ index.ts # Express server entry
â”‚ â”œâ”€â”€ routes/
â”‚ â”‚ â”œâ”€â”€ video.routes.ts # Upload & status APIs
â”‚ â”‚ â””â”€â”€ search.routes.ts # Query API


â”‚ â”œâ”€â”€ controllers/
â”‚ â”‚ â””â”€â”€ video.controller.ts
â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â”œâ”€â”€ transcript.service.ts # Whisper ASR logic
â”‚ â”‚ â”œâ”€â”€ embed.service.ts # Hugging Face embedding service
â”‚ â”‚ â”œâ”€â”€ vector.service.ts # Qdrant client logic
â”‚ â”‚ â””â”€â”€ storage.service.ts # Cloudflare R2 integration
â”‚ â”œâ”€â”€ utils/
â”‚ â”‚ â”œâ”€â”€ extractAudio.ts
â”‚ â”‚ â””â”€â”€ chunkText.ts
â”‚ â”œâ”€â”€ config/
â”‚ â”‚ â”œâ”€â”€ env.ts
â”‚ â”‚ â””â”€â”€ qdrant.ts
â”‚ â””â”€â”€ types/
â”‚ â””â”€â”€ video.types.ts
â”œâ”€â”€ uploads/
â”œâ”€â”€ .env.example
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸ§  System Architecture

![System Architecture](https://your-image-host.com/architecture.png)

## âš™ï¸ API Endpoints

### 1ï¸âƒ£ Health Check

**GET** `/api/health`  
- Response:

```json
{ "status": "ok", "message": "Server running" }
```

### 2ï¸âƒ£ Upload Video

**POST** /api/upload-video
multipart/form-data with a file field.
- Flow:
    - Save file locally.
    - Extract audio via ffmpeg.
    - Transcribe with Whisper.
    - Embed transcript chunks and store in Qdrant.

- Response:

```json
Copy code
{
  "videoId": "abc123",
  "message": "Video uploaded and indexed",
  "summary": "First few words of transcript..."
}
``` 

### 3ï¸âƒ£ Semantic Search

**GET** /api/search?videoId=<id>&q=<query>
- Returns timestamped transcript segments matching the query.
- Response:

```json
Copy code
{
  "videoId": "abc123",
  "query": "explain gradient descent",
  "results": [
    { "startTime": 123.4, "endTime": 150.0, "text": "...", "score": 0.92 },
    { "startTime": 210.0, "endTime": 240.0, "text": "...", "score": 0.89 }
  ]
}
```

## ğŸ§© Setup Instructions

### 1. Clone & Install

```bash
Copy code
git clone <repo-url>
cd backend
npm install
```

### 2. Configure Environment Variables

Fill with your api keys:

```bash
PORT=5000
HF_API_KEY=hf_xxx
QDRANT_URL=https://your-instance.qdrant.tech
QDRANT_API_KEY=your_api_key
R2_BUCKET_NAME=video-storage
R2_ACCESS_KEY_ID=your_access_key
R2_SECRET_ACCESS_KEY=your_secret_key
MONGO_URI=mongodb+srv://...
```

### 3. Start Server
```bash
npm run dev
```

### 4. Test in Postman
**GET** /api/health  
**POST** /api/upload-video  
**GET** /api/search?videoId=abc123&q=your+query  
